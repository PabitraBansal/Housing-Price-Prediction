{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing_Remove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWvuPn3PEUIn"
      },
      "source": [
        "#Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdvpjSTzEbub"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install xgboost\n",
        "!pip install pickle\n",
        "!pip install scipy\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44TscCFVXoG"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Na6ZsXbUa4E"
      },
      "source": [
        "import math\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from xgboost import XGBRegressor\n",
        "from scipy import stats,special\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA,TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPCcCGBxUj9D"
      },
      "source": [
        "# There are 81 columns: 79 Features + Id + SalePrice\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeAHoT9sUnNG"
      },
      "source": [
        "# Separating numerical and categorical data\n",
        "numerical_features = dataset.dtypes[dataset.dtypes != \"object\"].index\n",
        "print(\"Number of Numerical features: \", len(numerical_features))\n",
        "\n",
        "categorical_features = dataset.dtypes[dataset.dtypes == \"object\"].index\n",
        "print(\"Number of Categorical features: \", len(categorical_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZQLhsLhUrZM"
      },
      "source": [
        "print(dataset[numerical_features].columns)\n",
        "print(\"*\"*100)\n",
        "print(dataset[categorical_features].columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPoHq0ovUvMS"
      },
      "source": [
        "# List of features with missing values\n",
        "total = dataset.isnull().sum().sort_values(ascending=False)\n",
        "percent = (dataset.isnull().sum()/dataset.isnull().count()).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent*100], axis=1, keys=['Total', 'Percent'])\n",
        "missing_data.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jqOUxyoU44D"
      },
      "source": [
        "# There are 19 features having missing values \n",
        "# 16 Categorical\n",
        "# 3 Numerical\n",
        "categorical_null = []\n",
        "numerical_null = []\n",
        "for index,row in missing_data.iterrows():\n",
        "    if row['Total'] != 0:\n",
        "        if index in categorical_features:\n",
        "            categorical_null.append(index)\n",
        "        else:\n",
        "            numerical_null.append(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYIyadCQU8a9"
      },
      "source": [
        "print(categorical_null)\n",
        "print(\"*\"*100)\n",
        "print(numerical_null)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tasrdh8dU_He"
      },
      "source": [
        "# Sales price is not a normal distribution\n",
        "sns.set_style(\"white\")\n",
        "sns.set_color_codes(palette='deep')\n",
        "f, ax = plt.subplots(figsize=(8, 7))\n",
        "sns.distplot(dataset['SalePrice']);\n",
        "ax.xaxis.grid(False)\n",
        "ax.set(ylabel=\"Frequency\")\n",
        "ax.set(xlabel=\"SalePrice(in $)\")\n",
        "ax.set(title=\"SalePrice distribution\")\n",
        "sns.despine(trim=True, left=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1uYT5r_VER4"
      },
      "source": [
        "# visualising some more outliers in the data values\n",
        "fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\n",
        "plt.subplots_adjust(right=2)\n",
        "plt.subplots_adjust(top=2)\n",
        "for i, feature in enumerate(list(dataset[numerical_features]), 1):\n",
        "    plt.subplot(len(list(numerical_features)), 3, i)\n",
        "    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=dataset)\n",
        "        \n",
        "    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n",
        "    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n",
        "    \n",
        "    for j in range(2):\n",
        "        plt.tick_params(axis='x', labelsize=12)\n",
        "        plt.tick_params(axis='y', labelsize=12)\n",
        "    \n",
        "    plt.legend(loc='best', prop={'size': 10})\n",
        "        \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRs98aZLVH5l"
      },
      "source": [
        "# Categorical data and their unique value counts\n",
        "for catg in list(categorical_features) :\n",
        "    print(dataset[catg].value_counts())\n",
        "    print('#'*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551GURQuVSnd"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5i6AYaZVSVs"
      },
      "source": [
        "# Loading the both test and train set\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l01SFNQYVa8G"
      },
      "source": [
        "# Removing ID since it is unique for each datapoint\n",
        "train_ID = train['Id']\n",
        "test_ID = test['Id']\n",
        "train.drop(['Id'], axis=1, inplace=True)\n",
        "test.drop(['Id'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxxjV0MbVhj5"
      },
      "source": [
        "# Remove outliers\n",
        "cols = [feature for feature in numerical_features if feature != 'Id' and feature != 'SalePrice']\n",
        "Q1 = train[cols].quantile(0.01)\n",
        "Q99 = train[cols].quantile(0.99)\n",
        "train = train[~((train[cols] < (Q1)) |(train[cols] > (Q99))).any(axis=1)]\n",
        "\n",
        "# Replace with median\n",
        "\n",
        "train.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnevFiL5VlJ_"
      },
      "source": [
        "# Normalising the dependent variable\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "sns.set_style(\"white\")\n",
        "sns.set_color_codes(palette='deep')\n",
        "f, ax = plt.subplots(figsize=(8, 7))\n",
        "#Check the new distribution \n",
        "sns.distplot(train['SalePrice']);\n",
        "ax.xaxis.grid(False)\n",
        "ax.set(ylabel=\"Frequency\")\n",
        "ax.set(xlabel=\"SalePrice\")\n",
        "ax.set(title=\"SalePrice distribution\")\n",
        "sns.despine(trim=True, left=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBuZOSJ4Vn6m"
      },
      "source": [
        "# Split features and labels\n",
        "train_labels = train['SalePrice'].reset_index(drop=True)\n",
        "train_features = train.drop(['SalePrice'], axis=1)\n",
        "test_features = test\n",
        "\n",
        "# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\n",
        "all_features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
        "all_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZr7FyASVrIg"
      },
      "source": [
        "### Dealing with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Emclq7VutU"
      },
      "source": [
        "# Visualize missing values\n",
        "sns.set_style(\"white\")\n",
        "f, ax = plt.subplots(figsize=(8, 7))\n",
        "sns.set_color_codes(palette='deep')\n",
        "missing = round(all_features.isnull().mean()*100,2)\n",
        "missing = missing[missing > 0]\n",
        "missing.sort_values(inplace=True)\n",
        "missing.plot.bar(color=\"b\")\n",
        "# Tweak the visual presentation\n",
        "ax.xaxis.grid(False)\n",
        "ax.set(ylabel=\"Percent of missing values\")\n",
        "ax.set(xlabel=\"Features\")\n",
        "ax.set(title=\"Percent missing data by feature\")\n",
        "sns.despine(trim=True, left=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdNNW5M8dcOH"
      },
      "source": [
        "Check for missing attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om8YpaUcVyZF"
      },
      "source": [
        "all_features['Functional'] = all_features['Functional'].fillna('Typ')\n",
        "all_features['Electrical'] = all_features['Electrical'].fillna(\"SBrkr\")\n",
        "all_features['KitchenQual'] = all_features['KitchenQual'].fillna(\"TA\")\n",
        "\n",
        "# Replace the missing values in each of the columns below with their mode\n",
        "all_features['Exterior1st'] = all_features['Exterior1st'].fillna(all_features['Exterior1st'].mode()[0])\n",
        "all_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(all_features['Exterior2nd'].mode()[0])\n",
        "all_features['SaleType'] = all_features['SaleType'].fillna(all_features['SaleType'].mode()[0])\n",
        "all_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "# the data description stats that NA refers to \"No Pool\"\n",
        "all_features[\"PoolQC\"] = all_features[\"PoolQC\"].fillna(\"NA\")\n",
        "# Replacing the missing values with 0, since no garage = no cars in garage\n",
        "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
        "    all_features[col] = all_features[col].fillna(0)                              # Check for GarageYrBlt\n",
        "# Replacing the missing values with None\n",
        "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
        "    all_features[col] = all_features[col].fillna('None')\n",
        "# NaN values for these categorical basement features, means there's no basement\n",
        "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
        "    all_features[col] = all_features[col].fillna('None')\n",
        "# Replacing Nan value for SaleType as Other\n",
        "all_features['SaleType'] = all_features['SaleType'].fillna('Oth')\n",
        "all_features['FireplaceQu'] = all_features['FireplaceQu'].fillna('NA')\n",
        "all_features['Fence'] = all_features['Fence'].fillna('NA')\n",
        "all_features['Alley'] = all_features['Alley'].fillna('NA')\n",
        "all_features['MiscFeature'] = all_features['MiscFeature'].fillna('NA')\n",
        "\n",
        "# Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n",
        "all_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# We have no particular intuition around how to fill in the rest of the categorical features\n",
        "# So we replace their missing values with None\n",
        "objects = []\n",
        "for i in all_features.columns:\n",
        "    if all_features[i].dtype == object:\n",
        "        objects.append(i)\n",
        "all_features.update(all_features[objects].fillna('None'))\n",
        "    \n",
        "# And we do the same thing for numerical features, but this time with 0s\n",
        "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numeric = []\n",
        "for i in all_features.columns:\n",
        "    if all_features[i].dtype in numeric_dtypes:\n",
        "        numeric.append(i)\n",
        "all_features.update(all_features[numeric].fillna(0))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r-qhMXXV4qM"
      },
      "source": [
        "# Check if all the missing values have been dealt with or not\n",
        "data = pd.DataFrame(all_features)\n",
        "df_cols = list(pd.DataFrame(data))\n",
        "dict_x = {}\n",
        "for i in range(0, len(df_cols)):\n",
        "    dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n",
        "df_miss = sorted(dict_x.items(), key=lambda x: x[1], reverse=True)\n",
        "print('Percent of missing data')\n",
        "df_miss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5R4QX8R1ejx"
      },
      "source": [
        "# for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n",
        "#     all_features[feature] = all_features['YrSold'] - all_features[feature]\n",
        "#     all_features[feature < 0] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qUPNiCaV86Z"
      },
      "source": [
        "# Find skewed numerical features\n",
        "skew_features = all_features[[feature for feature in numerical_features if feature != 'Id' and feature != 'SalePrice']].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n",
        "\n",
        "high_skew = skew_features[skew_features > 0.5]\n",
        "skew_index = high_skew.index\n",
        "\n",
        "print(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\n",
        "skewness = pd.DataFrame({'Skew' :high_skew})\n",
        "skew_features.head(26)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ykPqn2QWAji"
      },
      "source": [
        "# We need a normal distribution since sklean library assumes the data to have a normal distribution and\n",
        "# does not perform well otherwise. Here we are trying to get a transformation which will give us a normal distribution.\n",
        "# For this we are using box - cox transform of 1 + x\n",
        "# y = ((1+x)**lmbda - 1) / lmbda  if lmbda != 0\n",
        "#     log(1+x)                    if lmbda == 0\n",
        "# lmbda is calculated using boxcox_normmax\n",
        "\n",
        "for i in skew_index:\n",
        "    all_features[i] = special.boxcox1p(all_features[i], stats.boxcox_normmax(all_features[i] + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHddJ26uWGrl"
      },
      "source": [
        "num_features = all_features.dtypes[all_features.dtypes != \"object\"].index\n",
        "for col in num_features:\n",
        "    skew = all_features[col].skew()\n",
        "    print('{:15}'.format(col), '{:05.2f}'.format(skew))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYsl8YBDWK7C"
      },
      "source": [
        "# One hot encoding ccategorical data because the models can only work with numerical data\n",
        "\n",
        "# Label encode for random forest\n",
        "all_features = pd.get_dummies(all_features).reset_index(drop=True)\n",
        "all_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA_W59hBWOuE"
      },
      "source": [
        "# Remove any duplicated column names\n",
        "all_features = all_features.loc[:,~all_features.columns.duplicated()]\n",
        "all_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGnhhLmtYctK"
      },
      "source": [
        "# Split to test & train data\n",
        "X_train = all_features.iloc[:len(train_labels), :]\n",
        "X_test = all_features.iloc[len(train_labels):, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ7ame15Wn-h"
      },
      "source": [
        "# Heatmap\n",
        "data = X_train.copy()[[feature for feature in num_features if feature != 'Id' and feature != 'SalePrice']].join(train_labels)\n",
        "nr_feats=len(data.columns)\n",
        "\n",
        "corr = data.corr()\n",
        "corr_abs = corr.abs()\n",
        "cols = corr_abs.nlargest(nr_feats, 'SalePrice')['SalePrice'].index\n",
        "cm = np.corrcoef(data[cols].values.T)\n",
        "\n",
        "plt.figure(figsize=(nr_feats/1.5, nr_feats/1.5))\n",
        "sns.set(font_scale=1.25)\n",
        "sns.heatmap(cm, linewidths=1.5, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, \n",
        "            yticklabels=cols.values, xticklabels=cols.values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv7By3lAhiyO"
      },
      "source": [
        "Remove correrated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1mMP9DDWRuH"
      },
      "source": [
        "# Features with strong correlation to SalePrice\n",
        "corr = X_train.copy().join(train_labels).corr()\n",
        "corr_abs = corr.abs()\n",
        "\n",
        "ser_corr = corr_abs.nlargest(len(X_train.columns), 'SalePrice')['SalePrice']\n",
        "\n",
        "cols_abv_corr_limit = list(ser_corr[ser_corr.values > 0.3].index)\n",
        "cols_bel_corr_limit = list(ser_corr[ser_corr.values <= 0.3].index)\n",
        "\n",
        "print(ser_corr.head(30))\n",
        "print(\"*\"*30)\n",
        "print(\"List of numerical features with r above 0.3 :\")\n",
        "print(cols_abv_corr_limit)\n",
        "print(\"*\"*30)\n",
        "print(\"List of numerical features with r below 0.3 :\")\n",
        "print(cols_bel_corr_limit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybpHpk0ZbZAM"
      },
      "source": [
        "Variance vs Components Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOpih5fXbg2a"
      },
      "source": [
        "def plot(dim_red):\n",
        "  plt.plot(np.cumsum(dim_red.explained_variance_ratio_))\n",
        "  plt.xlabel('number of components')\n",
        "  plt.ylabel('cumulative explained variance');\n",
        "  plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EDgqV7Qipck"
      },
      "source": [
        "Getting feature set using different"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJE8Im7kiZ4"
      },
      "source": [
        "# Correlation \n",
        "X_train_corr = X_train[[f for f in cols_abv_corr_limit if f != \"SalePrice\"]]\n",
        "X_test_corr = X_test[[f for f in cols_abv_corr_limit if f != \"SalePrice\"]]\n",
        "X_train_corr.to_csv('x_train_corr_removed.csv', index=False)\n",
        "X_test_corr.to_csv('x_test_corr_removed.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0xYahTd-yC5"
      },
      "source": [
        "# PCA\n",
        "pca = PCA(n_components = 0.99998, svd_solver = 'full')\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "pd.DataFrame(X_train_pca).to_csv('x_train_pca_removed.csv', index=False)\n",
        "pd.DataFrame(X_test_pca).to_csv('x_test_pca_removed.csv', index=False)\n",
        "plot(pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm9l8_LH-yeJ"
      },
      "source": [
        "# SVD\n",
        "svd = TruncatedSVD(50)\n",
        "X_train_svd = svd.fit_transform(X_train)\n",
        "X_test_svd = svd.transform(X_test)\n",
        "pd.DataFrame(X_train_corr).to_csv('x_train_svd_removed.csv', index=False)\n",
        "pd.DataFrame(X_test_corr).to_csv('x_test_svd_removed.csv', index=False)\n",
        "plot(svd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QctH-oJl-Axi"
      },
      "source": [
        "y_train = train_labels\n",
        "y_train.to_csv('y_train_removed.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y61NUY7QNrM9"
      },
      "source": [
        "### Time to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vh66VslIYf"
      },
      "source": [
        "def timeToTrain(model, X, y):\n",
        "  startTime = time.time()\n",
        "  model.fit(X,y)\n",
        "  endTime = time.time()\n",
        "  return endTime - startTime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfpvH2aAVDy0"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4WVAiVVF-0"
      },
      "source": [
        "def get_metrics(X_test, y_test, y_pred):\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  adj_r2 = 1 - ((1 - r2) * ((X_test.shape[0]-1) / (X_test.shape[0]-X_test.shape[1]-1)))\n",
        "\n",
        "  print(\"\\nMetrics\")\n",
        "  print(\"MSE =\", mean_squared_error(y_test, y_pred))\n",
        "  print(\"RMSE =\", math.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "  print(\"MAE =\", mean_absolute_error(y_test, y_pred))\n",
        "  print(\"R2 Score =\", r2)\n",
        "  print(\"Adjusted R2 Score =\", adj_r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4wzV2Aty8Ey"
      },
      "source": [
        "###Save and Load Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxLzmdRVy5kf"
      },
      "source": [
        "def saveModel(model, filename):\n",
        "    # save the model to disk\n",
        "    pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "def loadModel(filename):\n",
        "    # load the model from disk\n",
        "    loaded_model = pickle.load(open(filename, 'rb'))\n",
        "    return loaded_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqpvQ9mUiwce"
      },
      "source": [
        "# Training Different Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGqe8c84QmcF"
      },
      "source": [
        "### Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtB4xRedQoOB"
      },
      "source": [
        "def RidgeRegression(X, y, X__test, fileName):\n",
        "  ridge = Ridge(max_iter=3000, tol=0.2)\n",
        "  alphas = np.logspace(-4, 4, 40)\n",
        "  parameters = {'alpha': alphas}\n",
        "  clf = GridSearchCV(ridge, parameters)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  y__pred = clf.predict(X__test)\n",
        "  y__pred = np.exp(y__pred)\n",
        "\n",
        "  count = 0\n",
        "  with open(\"test.csv\",'r') as file:\n",
        "    newfile = open(fileName,'w');\n",
        "    data = csv.reader(file)\n",
        "    for row in data:\n",
        "        if row[0] == \"Id\":\n",
        "            newfile.write(\"Id,SalePrice\\n\")\n",
        "        else:\n",
        "            newfile.write(row[0]+\",\"+str(y__pred[count])+\"\\n\")\n",
        "            count += 1\n",
        "    newfile.close()\n",
        "\n",
        "  print(\"Best Parameter: \", clf.best_estimator_)\n",
        "  print('Feature Coefficients: ', clf.best_estimator_.coef_)\n",
        "  get_metrics(X_test, y_test, y_pred)\n",
        "  print('Time:', timeToTrain(clf.best_estimator_,X_train,y_train), 'sec')\n",
        "  print()\n",
        "\n",
        "  return clf.best_estimator_\n",
        "\n",
        "# Correlation Ridge Regression Score\n",
        "ridgeRegressionRemoveModelCorr = RidgeRegression(X_train_corr, y_train, X_test_corr, 'prediction_remove_ridge_corr.csv')\n",
        "saveModel(ridgeRegressionRemoveModelCorr, 'ridgeRegressionRemoveModelCorr')\n",
        "\n",
        "# PCA Ridge Regression Score\n",
        "ridgeRegressionRemoveModelPCA = RidgeRegression(X_train_pca, y_train, X_test_pca, 'prediction_remove_ridge_pca.csv')\n",
        "saveModel(ridgeRegressionRemoveModelPCA, 'ridgeRegressionRemoveModelPCA')\n",
        "\n",
        "# SVD Ridge Regression Score\n",
        "ridgeRegressionRemoveModelSVD = RidgeRegression(X_train_svd, y_train, X_test_svd, 'prediction_remove_ridge_svd.csv')\n",
        "saveModel(ridgeRegressionRemoveModelSVD, 'ridgeRegressionRemoveModelSVD')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIw_k9ME5ua-"
      },
      "source": [
        "###Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SPqgn3A5uwy"
      },
      "source": [
        "def LassoRegression(X, y, X__test, fileName):\n",
        "  lasso = Lasso(max_iter= 3000, tol = 0.2)\n",
        "  alphas = np.logspace(-4, 4, 40)\n",
        "  parameters = {'alpha': alphas}\n",
        "  clf = GridSearchCV(lasso, parameters)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  y__pred = clf.predict(X__test)\n",
        "  y__pred = np.exp(y__pred)\n",
        "\n",
        "  count = 0\n",
        "  with open(\"test.csv\",'r') as file:\n",
        "    newfile = open(fileName,'w');\n",
        "    data = csv.reader(file)\n",
        "    for row in data:\n",
        "        if row[0] == \"Id\":\n",
        "            newfile.write(\"Id,SalePrice\\n\")\n",
        "        else:\n",
        "            newfile.write(row[0]+\",\"+str(y__pred[count])+\"\\n\")\n",
        "            count += 1\n",
        "    newfile.close()\n",
        "\n",
        "  print(\"Best Parameter: \", clf.best_estimator_)\n",
        "  print('Feature Coefficients: ', clf.best_estimator_.coef_)\n",
        "  get_metrics(X_test, y_test, y_pred)\n",
        "  print('Time:', timeToTrain(clf.best_estimator_,X_train,y_train), 'sec')\n",
        "  print()\n",
        "\n",
        "  return clf.best_estimator_\n",
        "\n",
        "# Correlation Ridge Regression Score\n",
        "lassoRegressionRemoveModelCorr = LassoRegression(X_train_corr, y_train, X_test_corr, 'prediction_remove_lasso_corr.csv')\n",
        "saveModel(lassoRegressionRemoveModelCorr, \"lassoRegressionRemoveModelCorr\")\n",
        "\n",
        "# PCA Ridge Regression Score\n",
        "lassoRegressionRemoveModelPCA = LassoRegression(X_train_pca, y_train, X_test_pca, 'prediction_remove_lasso_pca.csv')\n",
        "saveModel(lassoRegressionRemoveModelPCA, 'lassoRegressionRemoveModelPCA')\n",
        "\n",
        "# SVD Ridge Regression Score\n",
        "lassoRegressionRemoveModelSVD = LassoRegression(X_train_svd, y_train, X_test_svd, 'prediction_remove_lasso_svd.csv')\n",
        "saveModel(lassoRegressionRemoveModelSVD, \"lassoRegressionRemoveModelSVD\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLPqP5698rSn"
      },
      "source": [
        "### Elastic Net Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vBYHQAS8trf"
      },
      "source": [
        "def ElasticNetRegression(X, y, X__test, fileName):\n",
        "  elasticNet = ElasticNet(max_iter=3000, tol=0.2)\n",
        "  l1_ratioArr = [] #Array of l1_ratios which decides the contribution of l1 and l2 error\n",
        "  for i in range(1, 11, 1):\n",
        "    l1_ratioArr.append(i * 0.1)\n",
        "  \n",
        "  alphas = np.logspace(-4, 4, 40)\n",
        "  parameters = {'l1_ratio': l1_ratioArr, 'alpha': alphas}\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  #By default cv = 5 and refit = true\n",
        "  clf = GridSearchCV(estimator = elasticNet, param_grid = parameters)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  y__pred = clf.predict(X__test)\n",
        "  y__pred = np.exp(y__pred)\n",
        "\n",
        "  count = 0\n",
        "  with open(\"test.csv\",'r') as file:\n",
        "    newfile = open(fileName,'w');\n",
        "    data = csv.reader(file)\n",
        "    for row in data:\n",
        "        if row[0] == \"Id\":\n",
        "            newfile.write(\"Id,SalePrice\\n\")\n",
        "        else:\n",
        "            newfile.write(row[0]+\",\"+str(y__pred[count])+\"\\n\")\n",
        "            count += 1\n",
        "    newfile.close()\n",
        "\n",
        "  print(\"Best Parameter: \", clf.best_estimator_)\n",
        "  print('Feature Coefficients: ', clf.best_estimator_.coef_)\n",
        "  get_metrics(X_test, y_test, y_pred)\n",
        "  print('Time:', timeToTrain(clf.best_estimator_,X_train,y_train), 'sec')\n",
        "  print()\n",
        "\n",
        "  return clf.best_estimator_\n",
        "\n",
        "# Correlation ElasticNet Regression Score\n",
        "elasticnetRegressionRemoveModelCorr = ElasticNetRegression(X_train_corr, y_train, X_test_corr, 'prediction_remove_elastic_corr.csv')\n",
        "saveModel(elasticnetRegressionRemoveModelCorr, \"elasticnetRegressionRemoveModelCorr\")\n",
        "\n",
        "# PCA ElasticNet Regression Score\n",
        "elasticnetRegressionRemoveModelPCA = ElasticNetRegression(X_train_pca, y_train, X_test_pca, 'prediction_remove_elastic_pca.csv')\n",
        "saveModel(elasticnetRegressionRemoveModelPCA, \"elasticnetRegressionRemoveModelPCA\")\n",
        "\n",
        "# SVD ElasticNet Regression Score\n",
        "elasticnetRegressionRemoveModelSVD = ElasticNetRegression(X_train_svd, y_train, X_test_svd, 'prediction_remove_elastic_svd.csv')\n",
        "saveModel(elasticnetRegressionRemoveModelSVD, \"elasticnetRegressionRemoveModelSVD\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_bFeClPaaPM"
      },
      "source": [
        "### Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDugHYFqYuR2"
      },
      "source": [
        "def RandomForestRegression(X, y, X__test, fileName):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Number of trees in random forest\n",
        "  # [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
        "  n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "\n",
        "  # Different criterions, MSE and MAE\n",
        "  criterions = [\"mse\", \"mae\"]\n",
        "  \n",
        "  # Number of features to consider at every split\n",
        "  max_features = ['auto', 'sqrt']\n",
        "  \n",
        "  # Maximum number of levels in tree\n",
        "  max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "  max_depth.append(None)\n",
        "\n",
        "  # Minimum number of samples required to split a node\n",
        "  min_samples_split = [2, 5, 10]\n",
        "  \n",
        "  # Minimum number of samples required at each leaf node\n",
        "  min_samples_leaf = [1, 2, 4]\n",
        "  \n",
        "  # Method of selecting samples for training each tree\n",
        "  bootstrap = [True]\n",
        "  \n",
        "  # Create the random grid\n",
        "  random_grid = {\n",
        "                'n_estimators': n_estimators,  \n",
        "                'criterion': criterions,\n",
        "                'max_features': max_features,\n",
        "                'max_depth': max_depth,\n",
        "                'min_samples_split': min_samples_split,\n",
        "                'min_samples_leaf': min_samples_leaf,\n",
        "                'bootstrap': bootstrap\n",
        "                 }\n",
        "\n",
        "  rf = RandomForestRegressor()\n",
        "  rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 40, random_state = 42)\n",
        "  rf_random.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = rf_random.predict(X_test)\n",
        "\n",
        "  y__pred = rf_random.predict(X__test)\n",
        "  y__pred = np.exp(y__pred)\n",
        "\n",
        "  count = 0\n",
        "  with open(\"test.csv\",'r') as file:\n",
        "    newfile = open(fileName,'w');\n",
        "    data = csv.reader(file)\n",
        "    for row in data:\n",
        "        if row[0] == \"Id\":\n",
        "            newfile.write(\"Id,SalePrice\\n\")\n",
        "        else:\n",
        "            newfile.write(row[0]+\",\"+str(y__pred[count])+\"\\n\")\n",
        "            count += 1\n",
        "    newfile.close()\n",
        "\n",
        "  print(\"Best Parameter: \", rf_random.best_estimator_)\n",
        "  get_metrics(X_test, y_test, y_pred)\n",
        "  print('Time:', timeToTrain(rf_random.best_estimator_,X_train,y_train), 'sec')\n",
        "  print()\n",
        "\n",
        "  return rf_random.best_estimator_\n",
        "\n",
        "# Correlation Random Forerst Regression Score\n",
        "randomForestRemoveModelCorr = RandomForestRegression(X_train_corr, y_train, X_test_corr, 'prediction_remove_rf_corr.csv')\n",
        "saveModel(randomForestRemoveModelCorr, \"randomForestRemoveModelCorr\")\n",
        "\n",
        "# PCA Random Forerst Regression Score\n",
        "randomForestRemoveModelPCA = RandomForestRegression(X_train_pca, y_train, X_test_pca, 'prediction_remove_rf_pca.csv')\n",
        "saveModel(randomForestRemoveModelPCA, \"randomForestRemoveModelPCA\")\n",
        "\n",
        "# SVD Random Forerst Regression Score\n",
        "randomForestRemoveModelSVD = RandomForestRegression(X_train_svd, y_train, X_test_svd, 'prediction_remove_rf_svd.csv')\n",
        "saveModel(randomForestRemoveModelSVD, \"randomForestRemoveModelSVD\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fyzW5d9xizs"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc-frRNrYsUa"
      },
      "source": [
        "def SVMRegression(X, y, X__test, fileName):\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Create the parameter grid\n",
        "  param_grid = {'C': [0.01, 0.05, 0.1, 0.2, 0.5, 0.9, 1, 1.2, 1.5 , 2, 5]}\n",
        "  \n",
        "  svr = SVR()\n",
        "  clf =  GridSearchCV(estimator = SVR(), param_grid = param_grid, refit = True)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  y__pred = clf.predict(X__test)\n",
        "  y__pred = np.exp(y__pred)\n",
        "\n",
        "  count = 0\n",
        "  with open(\"test.csv\",'r') as file:\n",
        "    newfile = open(fileName,'w');\n",
        "    data = csv.reader(file)\n",
        "    for row in data:\n",
        "        if row[0] == \"Id\":\n",
        "            newfile.write(\"Id,SalePrice\\n\")\n",
        "        else:\n",
        "            newfile.write(row[0]+\",\"+str(y__pred[count])+\"\\n\")\n",
        "            count += 1\n",
        "    newfile.close()\n",
        "\n",
        "  print(\"Best Parameter: \", clf.best_estimator_)\n",
        "  get_metrics(X_test, y_test, y_pred)\n",
        "  print('Time:', timeToTrain(svr,X_train,y_train), 'sec')\n",
        "  print()\n",
        "\n",
        "  return clf.best_estimator_\n",
        "\n",
        "# Correlation SVM Regression Score\n",
        "svmRemoveModelCorr = SVMRegression(X_train_corr, y_train, X_test_corr, 'prediction_remove_svm_corr.csv')\n",
        "saveModel(svmRemoveModelCorr, \"svmRemoveModelCorr\")\n",
        "\n",
        "# PCA SVM Regression Score\n",
        "svmRemoveModelPCA = SVMRegression(X_train_pca, y_train, X_test_pca, 'prediction_remove_svm_pca.csv')\n",
        "saveModel(svmRemoveModelPCA, \"svmRemoveModelPCA\")\n",
        "\n",
        "# SVD SVM Regression Score\n",
        "svmRemoveModelSVD = SVMRegression(X_train_svd, y_train, X_test_svd, 'prediction_remove_svm_svd.csv')\n",
        "saveModel(svmRemoveModelSVD, \"svmRemoveModelSVD\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5r_UZ56wmuL"
      },
      "source": [
        "## XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFURCUDrwtOH"
      },
      "source": [
        "def XGBoost(X, y, X__test, fileName):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Create the parameter grid\n",
        "  param_grid = {'learning_rate': [0.05, 0.1, 0.2, 0.3, 0.4, 0.5],  \n",
        "              'gamma': [10, 5, 1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'max_depth': [2,3,4,5,6,7]}  \n",
        "  \n",
        "  xgboostRegressor = XGBRegressor(objective = 'reg:squarederror')\n",
        "  clf = GridSearchCV(estimator = xgboostRegressor, param_grid = param_grid, refit = True)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "  y__pred = clf.predict(X__test)\n",
        "  y__pred = np.exp(y__pred)\n",
        "  \n",
        "  count = 0\n",
        "  with open(\"test.csv\",'r') as file:\n",
        "    newfile = open(fileName,'w');\n",
        "    data = csv.reader(file)\n",
        "    for row in data:\n",
        "        if row[0] == \"Id\":\n",
        "            newfile.write(\"Id,SalePrice\\n\")\n",
        "        else:\n",
        "            newfile.write(row[0]+\",\"+str(y__pred[count])+\"\\n\")\n",
        "            count += 1\n",
        "    newfile.close()\n",
        "\n",
        "  print(\"Best Parameter: \", clf.best_estimator_)\n",
        "  get_metrics(X_test, y_test, y_pred)\n",
        "  print('Time:', timeToTrain(xgboostRegressor,X_train,y_train), 'sec')\n",
        "  print()\n",
        "\n",
        "  return clf.best_estimator_\n",
        "\n",
        "# Correlation SVM Regression Score\n",
        "xgboostRemoveModelCorr = XGBoost(X_train_corr, y_train, X_test_corr, 'prediction_remove_xgBoost_corr.csv')\n",
        "saveModel(xgboostRemoveModelCorr, \"xgboostRemoveModelCorr\")\n",
        "\n",
        "# PCA SVM Regression Score\n",
        "xgboostRemoveModelPCA = XGBoost(X_train_pca, y_train, X_test_pca, 'prediction_remove_xgBoost_pca.csv')\n",
        "saveModel(xgboostRemoveModelPCA, \"xgboostRemoveModelPCA\")\n",
        "\n",
        "# SVD SVM Regression Score\n",
        "xgboostRemoveModelSVD = XGBoost(X_train_svd, y_train, X_test_svd, 'prediction_remove_xgBoost_svd.csv')\n",
        "saveModel(xgboostRemoveModelSVD, \"xgboostRemoveModelSVD\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}